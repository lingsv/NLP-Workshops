{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJbFuG-Ugv02"
   },
   "source": [
    "# Classificador\n",
    "\n",
    "Neste Workshop, vamos aprender como criar um classificador que diz quais músicas são da Rihanna e quais são da Beyoncé utilizando aprendizado de máquina, mais especificamente *Aprendizado Supervisionado*, uma das áreas de Machine Learning.\n",
    "\n",
    "Então vamos lá!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Qqq3M_vPW2p"
   },
   "source": [
    "## Importando os dados\n",
    "\n",
    "O primeiro passo é importar seus dados, no caso nosso Dataframe (como chamamos a *tabela* que guarda as informações que usaremos). Fazemos isso com a biblioteca `pandas`.\n",
    "\n",
    "Com a função `read_csv` lemos nosso arquivo e guardamos ele na váriavel `df`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DC8V9WPPW2w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dv9XVp4NPW25",
    "outputId": "db1785f4-8c19-4e92-d683-ad6ace15c439"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome da Música</th>\n",
       "      <th>link</th>\n",
       "      <th>album</th>\n",
       "      <th>letra</th>\n",
       "      <th>artista</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'03 Bonnie &amp; Clyde</td>\n",
       "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
       "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
       "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
       "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
       "      <td>BEYONCÉ</td>\n",
       "      <td>Your challengers are a young group from Housto...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
       "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1+1</td>\n",
       "      <td>/beyonce/11.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 Inch (Feat. The Weeknd)</td>\n",
       "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
       "      <td>LEMONADE</td>\n",
       "      <td>Six inch heels She walked in the club like nob...</td>\n",
       "      <td>Beyoncé</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nome da Música  \\\n",
       "0                            '03 Bonnie & Clyde   \n",
       "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)   \n",
       "2               ***Flawless (Feat. Nicki Minaj)   \n",
       "3                                           1+1   \n",
       "4                     6 Inch (Feat. The Weeknd)   \n",
       "\n",
       "                                                link  \\\n",
       "0                      /beyonce/03-bonnie-clyde.html   \n",
       "1  /beyonce/flawless-feat-chimamanda-ngozi-adichi...   \n",
       "2            /beyonce/flawless-feat-nicki-minaj.html   \n",
       "3                                   /beyonce/11.html   \n",
       "4               /beyonce/6-inch-feat-the-weeknd.html   \n",
       "\n",
       "                                               album  \\\n",
       "0  I Am... Yours: An Intimate Performance at Wynn...   \n",
       "1                                            BEYONCÉ   \n",
       "2                         BEYONCÉ [Platinum Edition]   \n",
       "3                         BEYONCÉ [Platinum Edition]   \n",
       "4                                           LEMONADE   \n",
       "\n",
       "                                               letra  artista  \n",
       "0  Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...  Beyoncé  \n",
       "1  Your challengers are a young group from Housto...  Beyoncé  \n",
       "2  Dum-da-de-da Do, do, do, do, do, do (Coming do...  Beyoncé  \n",
       "3  If I ain't got nothing I got you If I ain't go...  Beyoncé  \n",
       "4  Six inch heels She walked in the club like nob...  Beyoncé  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('beyonce_rihanna.csv', index_col=0)\n",
    "\n",
    "# vamos explorar nosso dataframe olhando apenas as primeiras linhas com a função abaixo:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a63DRicqPW3G"
   },
   "source": [
    "## Pré-processamentos\n",
    "\n",
    "Antes de partir para o aprendizado de máquina, precisamos preparar nosso texto. Fazemos isso porque, para a máquina, algumas palavras ou estruturas do nosso texto não importam e não fazem diferença. \n",
    "São muitos os métodos de pré-processamento, mas aqui vamos realizar apenas alguns: \n",
    "* Tokenização\n",
    "* Remover stopwords\n",
    "* Deixar todo o texto em minúsculo\n",
    "* Selecionar apenas letras com REGEX\n",
    "* Lemmatização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poFLBHOcPW3J",
    "outputId": "a74b3c99-759e-4621-b88a-5a40b534dc40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here I am Looking in the mirror An open face, the pain erased Now the sky is clearer I can see the sun Now that all is, all is said and done, oh  There you are Always strong when I need you You let me give And now I live, fearless and protected With the one I will love After all is, all is said and done  I once believed that hearts were made to bleed (Inside I once believed that hearts were made to bleed, oh baby) But now I'm not afraid to say I need you, I need you so stay with me  These precious (precious) hours (yeah) Greet each dawn in open arms And dream, into tomorrow  Where there's only love After all is, all is said and done  (Yeah baby) Oh baby (Inside I once believed, That hearts were meant to bleed)  (I'll never) I'll never be afraid to say I need you, I need you, so here  Here we are in the still of this moment Fear is gone, hope lives on  We found our happy ending For there's only love (only love) And this sweet, sweet love After all is, all is said and done  Yeah baby after all is (all is)  All is said and done\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando uma música como exemplo\n",
    "exemplo = df['letra'][10]\n",
    "exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY_Ku-a_PW3b"
   },
   "source": [
    "### Tokenização \n",
    "\n",
    "Uma parte importante no pré-processamento de um texto é a tokenização. Isto é, transformar elementos do seu texto em tokens, ou seja, strings dentro de uma lista  -  ou, se você não tiver conhecimento de python, transformar todas as palavras do texto em elementos individuais separados por aspas. \n",
    "Podemos tokenizar palavras com `word_tokenize`, essa função recebe o texto como argumento e retorna todas as palavras do texto em forma de tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcXQSNTOPW3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/luisa-\n",
      "[nltk_data]     gaivota/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/luisa-\n",
      "[nltk_data]     gaivota/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgTkezQEPW3n",
    "outputId": "776a61d5-5363-4c0c-b4e4-a7691ad4b5ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Looking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mirror',\n",
       " 'An',\n",
       " 'open',\n",
       " 'face',\n",
       " ',',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'erased',\n",
       " 'Now',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'clearer',\n",
       " 'I',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'Now',\n",
       " 'that',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " ',',\n",
       " 'oh',\n",
       " 'There',\n",
       " 'you',\n",
       " 'are',\n",
       " 'Always',\n",
       " 'strong',\n",
       " 'when',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " 'You',\n",
       " 'let',\n",
       " 'me',\n",
       " 'give',\n",
       " 'And',\n",
       " 'now',\n",
       " 'I',\n",
       " 'live',\n",
       " ',',\n",
       " 'fearless',\n",
       " 'and',\n",
       " 'protected',\n",
       " 'With',\n",
       " 'the',\n",
       " 'one',\n",
       " 'I',\n",
       " 'will',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " '(',\n",
       " 'Inside',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " ',',\n",
       " 'oh',\n",
       " 'baby',\n",
       " ')',\n",
       " 'But',\n",
       " 'now',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'me',\n",
       " 'These',\n",
       " 'precious',\n",
       " '(',\n",
       " 'precious',\n",
       " ')',\n",
       " 'hours',\n",
       " '(',\n",
       " 'yeah',\n",
       " ')',\n",
       " 'Greet',\n",
       " 'each',\n",
       " 'dawn',\n",
       " 'in',\n",
       " 'open',\n",
       " 'arms',\n",
       " 'And',\n",
       " 'dream',\n",
       " ',',\n",
       " 'into',\n",
       " 'tomorrow',\n",
       " 'Where',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " '(',\n",
       " 'Yeah',\n",
       " 'baby',\n",
       " ')',\n",
       " 'Oh',\n",
       " 'baby',\n",
       " '(',\n",
       " 'Inside',\n",
       " 'I',\n",
       " 'once',\n",
       " 'believed',\n",
       " ',',\n",
       " 'That',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'bleed',\n",
       " ')',\n",
       " '(',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " ')',\n",
       " 'I',\n",
       " \"'ll\",\n",
       " 'never',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'I',\n",
       " 'need',\n",
       " 'you',\n",
       " ',',\n",
       " 'so',\n",
       " 'here',\n",
       " 'Here',\n",
       " 'we',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'still',\n",
       " 'of',\n",
       " 'this',\n",
       " 'moment',\n",
       " 'Fear',\n",
       " 'is',\n",
       " 'gone',\n",
       " ',',\n",
       " 'hope',\n",
       " 'lives',\n",
       " 'on',\n",
       " 'We',\n",
       " 'found',\n",
       " 'our',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'For',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'only',\n",
       " 'love',\n",
       " '(',\n",
       " 'only',\n",
       " 'love',\n",
       " ')',\n",
       " 'And',\n",
       " 'this',\n",
       " 'sweet',\n",
       " ',',\n",
       " 'sweet',\n",
       " 'love',\n",
       " 'After',\n",
       " 'all',\n",
       " 'is',\n",
       " ',',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'Yeah',\n",
       " 'baby',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " '(',\n",
       " 'all',\n",
       " 'is',\n",
       " ')',\n",
       " 'All',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizando a primeira música\n",
    "tokens = word_tokenize(exemplo)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8iX6icQ4PW34"
   },
   "source": [
    "### Selecionando apenas as letras e deixando todas em minúsculas\n",
    "\n",
    "Para a máquina, pontuações não são necessárias, por isso um pré-processamento necessário é selecionar apenas as letras de um texto. \n",
    "\n",
    "Porém, **antes disso** precisamos deixar todas as letras em minúsculo, não somente porque isso facilita a aplicação do REGEX, mas também porque a máquina tende a interpretar palavras com letras maiúsculas e minúsculas como sendo diferentes. Por exemplo, Beyoncé e beyoncé podem ser interpretadas como palavras distintas. Então vamos deixar as letras minúsculas com a função `.lower`.\n",
    "\n",
    "Feito isso, podemos selecionar apenas as letras com REGEX, mais especificamente com a função `re.findall`, que, além de retornar apenas as letras, já tokeniza o texto para você! \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWfHh-yfPW38",
    "outputId": "e1653025-f40f-428e-d8e2-7af711870b64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'i',\n",
       " 'am',\n",
       " 'looking',\n",
       " 'in',\n",
       " 'the',\n",
       " 'mirror',\n",
       " 'an',\n",
       " 'open',\n",
       " 'face',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'erased',\n",
       " 'now',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'clearer',\n",
       " 'i',\n",
       " 'can',\n",
       " 'see',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'now',\n",
       " 'that',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'oh',\n",
       " 'there',\n",
       " 'you',\n",
       " 'are',\n",
       " 'always',\n",
       " 'strong',\n",
       " 'when',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'you',\n",
       " 'let',\n",
       " 'me',\n",
       " 'give',\n",
       " 'and',\n",
       " 'now',\n",
       " 'i',\n",
       " 'live',\n",
       " 'fearless',\n",
       " 'and',\n",
       " 'protected',\n",
       " 'with',\n",
       " 'the',\n",
       " 'one',\n",
       " 'i',\n",
       " 'will',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'inside',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'made',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'oh',\n",
       " 'baby',\n",
       " 'but',\n",
       " 'now',\n",
       " 'i',\n",
       " 'm',\n",
       " 'not',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'stay',\n",
       " 'with',\n",
       " 'me',\n",
       " 'these',\n",
       " 'precious',\n",
       " 'precious',\n",
       " 'hours',\n",
       " 'yeah',\n",
       " 'greet',\n",
       " 'each',\n",
       " 'dawn',\n",
       " 'in',\n",
       " 'open',\n",
       " 'arms',\n",
       " 'and',\n",
       " 'dream',\n",
       " 'into',\n",
       " 'tomorrow',\n",
       " 'where',\n",
       " 'there',\n",
       " 's',\n",
       " 'only',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'yeah',\n",
       " 'baby',\n",
       " 'oh',\n",
       " 'baby',\n",
       " 'inside',\n",
       " 'i',\n",
       " 'once',\n",
       " 'believed',\n",
       " 'that',\n",
       " 'hearts',\n",
       " 'were',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'i',\n",
       " 'll',\n",
       " 'never',\n",
       " 'i',\n",
       " 'll',\n",
       " 'never',\n",
       " 'be',\n",
       " 'afraid',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'i',\n",
       " 'need',\n",
       " 'you',\n",
       " 'so',\n",
       " 'here',\n",
       " 'here',\n",
       " 'we',\n",
       " 'are',\n",
       " 'in',\n",
       " 'the',\n",
       " 'still',\n",
       " 'of',\n",
       " 'this',\n",
       " 'moment',\n",
       " 'fear',\n",
       " 'is',\n",
       " 'gone',\n",
       " 'hope',\n",
       " 'lives',\n",
       " 'on',\n",
       " 'we',\n",
       " 'found',\n",
       " 'our',\n",
       " 'happy',\n",
       " 'ending',\n",
       " 'for',\n",
       " 'there',\n",
       " 's',\n",
       " 'only',\n",
       " 'love',\n",
       " 'only',\n",
       " 'love',\n",
       " 'and',\n",
       " 'this',\n",
       " 'sweet',\n",
       " 'sweet',\n",
       " 'love',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done',\n",
       " 'yeah',\n",
       " 'baby',\n",
       " 'after',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'all',\n",
       " 'is',\n",
       " 'said',\n",
       " 'and',\n",
       " 'done']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "letras = re.findall(r'\\b[A-zÀ-úü]+\\b', exemplo.lower())\n",
    "letras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wn-sfEMSPW4a"
   },
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords são palavras que, apesar de muito frequentes, não são importantes/relevantes para a máquina. Entre elas, podemos encontrar artigos como “o” e “uma”, ou preposições como “de” e “em”, entre outras palavras frequentes no idioma. Para removê-las do texto, utilizamos uma lista de stopwords disponível na biblioteca NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RngDaWfjPW4f",
    "outputId": "9488d6ba-9fd8-4f4e-b0d0-a720d5ebb486"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "stops #lista de stopwords em inglês "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOwQ1ed6PW4n"
   },
   "source": [
    "Como remover stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWI0dceNPW4p",
    "outputId": "0a03e23d-a82f-43ab-d49f-90f0f6a24846"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looking mirror open face pain erased sky clearer see sun said done oh always strong need let give live fearless protected one love said done believed hearts made bleed inside believed hearts made bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love said done yeah baby oh baby inside believed hearts meant bleed never never afraid say need need still moment fear gone hope lives found happy ending love love sweet sweet love said done yeah baby said done'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_stopwords = [palavra for palavra in letras if palavra not in stops]\n",
    "palavras_importantes = \" \".join(sem_stopwords)\n",
    "palavras_importantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afGEg33WPW4z"
   },
   "source": [
    "### Lematização \n",
    "\n",
    "Assim como Stopwords, ter verbos conjugados em um texto não faz diferença quando a máquina vai processá-lo. Por isso, existem duas ferramentas chamadas Lemmatização e Stemmatização. Ambas fazem a mesma coisa: Quando passado um texto como argumento, elas reduzem todas as formas verbais conjugadas à sua raiz. A única diferença, entretanto, é que a função que lemmatiza seu texto reduz todos os verbos a forma verdadeira da raiz  -  por isso quanto maior seu texto, mais tempo essa função demora para rodar no código - , enquanto a função que stemmatiza apenas \"corta\" as palavras no meio usando a raiz como base, o que pode gerar palavras que não existem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGXq4zZiPW40"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spc = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSZ772ZqPW5A",
    "outputId": "6e3bf869-021d-4c0c-c424-32aa8c83d32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look mirror open face pain erase sky clearer see sun say do oh always strong need let give live fearless protect one love say do believe hearts make bleed inside believe hearts make bleed oh baby afraid say need need stay precious precious hours yeah greet dawn open arms dream tomorrow love say do yeah baby oh baby inside believe hearts mean bleed never never afraid say need need still moment fear go hope lives find happy end love love sweet sweet love say do yeah baby say do\n"
     ]
    }
   ],
   "source": [
    "spc_letras = spc(palavras_importantes)\n",
    "lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "texto_limpo = \" \".join(lemmas)\n",
    "print(texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYlDvyRSPW5q"
   },
   "source": [
    "Construa agora uma função para realizar todos os pré-processamentos ao invés de fazê-los um a um: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto(texto):\n",
    "    '''\n",
    "    Função para converter todas as letras para sua forma minúscula, selecionar apenas as letras,\n",
    "    remover stopwords e lematizar o texto.  \n",
    "    '''\n",
    "\n",
    "    ### Transforme as letras para minúscula ###\n",
    "    minusculas = texto.lower()\n",
    "    \n",
    "    ### Selecione apenas as letras do texto ##\n",
    "    letras = re.findall(r'\\b[A-zÀ-úü]+\\b', minusculas) \n",
    "    \n",
    "    ### Removendo as stopwords ###\n",
    "    stops = set(stopwords.words('english')) \n",
    "    # Retire as stopwords de letras\n",
    "    palavras_sem_stopwords = [w for w in letras if w not in stops]\n",
    "    # Junte as palavras sem stopwords \n",
    "    palavras_importantes = \" \".join(palavras_sem_stopwords) \n",
    "    \n",
    "    ### Lematização ###\n",
    "    spc_letras = spc(palavras_importantes)\n",
    "    # Lematize o texto \n",
    "    lemmas = [token.lemma_ if token.pos_ == 'VERB' else str(token) for token in spc_letras]\n",
    "    # Junte os lemmas \n",
    "    texto_limpo = \" \".join(lemmas)\n",
    "    \n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCW0gFxwpNPL"
   },
   "source": [
    "Agora vamos aplicá-la aos nossos dados, mais espcificamente na coluna \"letra\", que contém as músicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mrrNlaKPW6V"
   },
   "outputs": [],
   "source": [
    "df['Texto Limpo'] = df['letra'].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI9AD4--PW6p",
    "outputId": "dd613dbf-904b-43ee-e778-a5cbf10777c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nome da Música</th>\n",
       "      <th>link</th>\n",
       "      <th>album</th>\n",
       "      <th>letra</th>\n",
       "      <th>artista</th>\n",
       "      <th>Texto Limpo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'03 Bonnie &amp; Clyde</td>\n",
       "      <td>/beyonce/03-bonnie-clyde.html</td>\n",
       "      <td>I Am... Yours: An Intimate Performance at Wynn...</td>\n",
       "      <td>Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>jay z uh uh uh ready b let go get em look youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>***Flawless (Feat. Chimamanda Ngozi Adichie)</td>\n",
       "      <td>/beyonce/flawless-feat-chimamanda-ngozi-adichi...</td>\n",
       "      <td>BEYONCÉ</td>\n",
       "      <td>Your challengers are a young group from Housto...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>challengers young group houston welcome beyonc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***Flawless (Feat. Nicki Minaj)</td>\n",
       "      <td>/beyonce/flawless-feat-nicki-minaj.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>Dum-da-de-da Do, do, do, do, do, do (Coming do...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>dum da de da come dripping candy ground stay y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1+1</td>\n",
       "      <td>/beyonce/11.html</td>\n",
       "      <td>BEYONCÉ [Platinum Edition]</td>\n",
       "      <td>If I ain't got nothing I got you If I ain't go...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>get nothing get get something give damn cause ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6 Inch (Feat. The Weeknd)</td>\n",
       "      <td>/beyonce/6-inch-feat-the-weeknd.html</td>\n",
       "      <td>LEMONADE</td>\n",
       "      <td>Six inch heels She walked in the club like nob...</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>six inch heels walk club like nobody business ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nome da Música  \\\n",
       "0                            '03 Bonnie & Clyde   \n",
       "1  ***Flawless (Feat. Chimamanda Ngozi Adichie)   \n",
       "2               ***Flawless (Feat. Nicki Minaj)   \n",
       "3                                           1+1   \n",
       "4                     6 Inch (Feat. The Weeknd)   \n",
       "\n",
       "                                                link  \\\n",
       "0                      /beyonce/03-bonnie-clyde.html   \n",
       "1  /beyonce/flawless-feat-chimamanda-ngozi-adichi...   \n",
       "2            /beyonce/flawless-feat-nicki-minaj.html   \n",
       "3                                   /beyonce/11.html   \n",
       "4               /beyonce/6-inch-feat-the-weeknd.html   \n",
       "\n",
       "                                               album  \\\n",
       "0  I Am... Yours: An Intimate Performance at Wynn...   \n",
       "1                                            BEYONCÉ   \n",
       "2                         BEYONCÉ [Platinum Edition]   \n",
       "3                         BEYONCÉ [Platinum Edition]   \n",
       "4                                           LEMONADE   \n",
       "\n",
       "                                               letra  artista  \\\n",
       "0  Jay-z Uh-uh-uh You ready b? Let's go get 'em. ...  Beyoncé   \n",
       "1  Your challengers are a young group from Housto...  Beyoncé   \n",
       "2  Dum-da-de-da Do, do, do, do, do, do (Coming do...  Beyoncé   \n",
       "3  If I ain't got nothing I got you If I ain't go...  Beyoncé   \n",
       "4  Six inch heels She walked in the club like nob...  Beyoncé   \n",
       "\n",
       "                                         Texto Limpo  \n",
       "0  jay z uh uh uh ready b let go get em look youn...  \n",
       "1  challengers young group houston welcome beyonc...  \n",
       "2  dum da de da come dripping candy ground stay y...  \n",
       "3  get nothing get get something give damn cause ...  \n",
       "4  six inch heels walk club like nobody business ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # vamos ver como ficou?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o96vg7AZPW61"
   },
   "source": [
    "## Feature Extraction\n",
    "Antes de treinar o nosso modelo, precisamos organizar os nossos documentos em features que o computador consegue entender, assim, vamos precisamos transformar o nosso texto em algum tipo de representação numérica. Para isso, vamos usar o Bag of Words. \n",
    "\n",
    "### Bag of Words \n",
    "**O que é o Bag of Words?:** BoW é uma forma de representação de texto que descreve a ocorrência de palavras em um documento. Para o BoW a ordem não importa, essa forma de representação só se importa se as palavras conhecidas ocorrem ou não no documento (literalmente um \"saco\" de palavras). \n",
    "\n",
    "Para implementarmos o Bag of Words, precisamos de três coisas: \n",
    "1. Um vocabulário com as palavras conhecidas\n",
    "2. A ocorrência dessas palavras\n",
    "3. Formar vetores a partir dos documentos \n",
    "\n",
    "**Exemplo**\n",
    "\n",
    "\"to the left to the left everything you own in the box to the left\"\n",
    "\n",
    "1. Construir o vocabulário\n",
    "\n",
    "    [\"to\", \"the\", \"left\", \"everything\", \"you\", \"own\", \"in\", \"box\"]\n",
    "    \n",
    "\n",
    "2. Ocorrência das palavras\n",
    "\n",
    "    {\"to\": 3, \"the\": 3, \"left\":3, \"everything\":1, \"you\":1, \"own\":1, \"in\":1, \"box\":1}\n",
    "\n",
    "\n",
    "3. Vetores\n",
    "\n",
    "    Considerando que o nosso documento fosse: \"to the left\"\n",
    "\n",
    "    Usando o vocabulário que construímos antes, o nosso vetor seria: \n",
    "\n",
    "    [1, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ggonk5WPW64"
   },
   "source": [
    "### Count Vectorizer \n",
    "Felizmente, temos o CountVectorizer! Com ele, conseguimos implementar todos os passos acima de uma maneira bem simples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OoiGvcEYPW67"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Bag of words\n",
    "count_vectorizer = CountVectorizer(binary=True)\n",
    "X = count_vectorizer.fit_transform(df['Texto Limpo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpwp7cETPW7A"
   },
   "source": [
    "Olhando o nosso vocabulário: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vrncvpr2PW7A",
    "outputId": "b52bb5dc-387c-4dea-bc4e-57ed41d7300c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaaaaah',\n",
       " 'aaah',\n",
       " 'aah',\n",
       " 'aahhhh',\n",
       " 'aaron',\n",
       " 'abandon',\n",
       " 'abanenkani',\n",
       " 'abaziyo',\n",
       " 'abit',\n",
       " 'abita',\n",
       " 'able',\n",
       " 'aboard',\n",
       " 'abrasive',\n",
       " 'absolutely',\n",
       " 'abstain',\n",
       " 'abu',\n",
       " 'abunch',\n",
       " 'abuse',\n",
       " 'acabado',\n",
       " 'acabo',\n",
       " 'acabó',\n",
       " 'acaso',\n",
       " 'accent',\n",
       " 'accept',\n",
       " 'accepte',\n",
       " 'acceptin',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'accomodation',\n",
       " 'accomplishments',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accule',\n",
       " 'accusations',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'achetant',\n",
       " 'achieve',\n",
       " 'achètera',\n",
       " 'acompañarme',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activité',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acuerdo',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'address',\n",
       " 'adichie',\n",
       " 'adicto',\n",
       " 'adiós',\n",
       " 'adjust',\n",
       " 'adlibs',\n",
       " 'admire',\n",
       " 'admit',\n",
       " 'admittin',\n",
       " 'adolescent',\n",
       " 'adonde',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adorent',\n",
       " 'adrenaline',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advantage',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affectin',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affliction',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'afros',\n",
       " 'aftaparty',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agent',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agressive',\n",
       " 'aguanto',\n",
       " 'aguilera',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'aham',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhhh',\n",
       " 'ahhhhh',\n",
       " 'ahhhhhhh',\n",
       " 'ahn',\n",
       " 'ahold',\n",
       " 'ahora',\n",
       " 'ahuh',\n",
       " 'ahí',\n",
       " 'ai',\n",
       " 'aim',\n",
       " 'aimes',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airs',\n",
       " 'aisle',\n",
       " 'akhenaton',\n",
       " 'akon',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alabies',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarms',\n",
       " 'albino',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alejandro',\n",
       " 'alert',\n",
       " 'algebra',\n",
       " 'alguien',\n",
       " 'ali',\n",
       " 'alicia',\n",
       " 'alight',\n",
       " 'align',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'allan',\n",
       " 'allegiance',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'alley',\n",
       " 'allez',\n",
       " 'alligators',\n",
       " 'allow',\n",
       " 'alma',\n",
       " 'almighty',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'altercation',\n",
       " 'although',\n",
       " 'alumna',\n",
       " 'always',\n",
       " 'amagwali',\n",
       " 'amanda',\n",
       " 'amandla',\n",
       " 'amante',\n",
       " 'amaphi',\n",
       " 'amaqhawe',\n",
       " 'amar',\n",
       " 'amarte',\n",
       " 'amaze',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'ambas',\n",
       " 'ambition',\n",
       " 'ambulance',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amigos',\n",
       " 'amiss',\n",
       " 'amistad',\n",
       " 'amo',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amor',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amour',\n",
       " 'amp',\n",
       " 'amplify',\n",
       " 'amwin',\n",
       " 'amélioration',\n",
       " 'analogy',\n",
       " 'ancestors',\n",
       " 'anda',\n",
       " 'andasians',\n",
       " 'andoyiki',\n",
       " 'andre',\n",
       " 'ange',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'anges',\n",
       " 'animal',\n",
       " 'ankh',\n",
       " 'ankles',\n",
       " 'anna',\n",
       " 'annie',\n",
       " 'annonce',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'answer',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'antidote',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'aorte',\n",
       " 'apart',\n",
       " 'apollo',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appease',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'applause',\n",
       " 'apply',\n",
       " 'appreciate',\n",
       " 'approach',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'après',\n",
       " 'aps',\n",
       " 'aquarian',\n",
       " 'aquarius',\n",
       " 'aqueducts',\n",
       " 'aquella',\n",
       " 'aquí',\n",
       " 'ardeur',\n",
       " 'ardor',\n",
       " 'area',\n",
       " 'arena',\n",
       " 'arenas',\n",
       " 'argu',\n",
       " 'argue',\n",
       " 'arguement',\n",
       " 'arie',\n",
       " 'aries',\n",
       " 'aristocracy',\n",
       " 'arm',\n",
       " 'armada',\n",
       " 'armand',\n",
       " 'armani',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrest',\n",
       " 'arrive',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrosent',\n",
       " 'ars',\n",
       " 'art',\n",
       " 'arthur',\n",
       " 'article',\n",
       " 'articulate',\n",
       " 'artist',\n",
       " 'asco',\n",
       " 'ashamed',\n",
       " 'ashanti',\n",
       " 'ashes',\n",
       " 'ashley',\n",
       " 'ashtray',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asleep',\n",
       " 'asphalt',\n",
       " 'aspiration',\n",
       " 'aspire',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'assemblage',\n",
       " 'asses',\n",
       " 'asshole',\n",
       " 'assume',\n",
       " 'assure',\n",
       " 'astaire',\n",
       " 'aswe',\n",
       " 'así',\n",
       " 'atch',\n",
       " 'atención',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atmosphere',\n",
       " 'atomic',\n",
       " 'attach',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'attendance',\n",
       " 'attendant',\n",
       " 'attention',\n",
       " 'attire',\n",
       " 'attirent',\n",
       " 'attitude',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'au',\n",
       " 'audacity',\n",
       " 'audemars',\n",
       " 'audibles',\n",
       " 'auditionin',\n",
       " 'audmars',\n",
       " 'audubon',\n",
       " 'aunque',\n",
       " 'aura',\n",
       " 'austin',\n",
       " 'aut',\n",
       " 'automne',\n",
       " 'autre',\n",
       " 'autres',\n",
       " 'aux',\n",
       " 'avail',\n",
       " 'avant',\n",
       " 'ave',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'aveugle',\n",
       " 'avi',\n",
       " 'aviator',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'awahlangani',\n",
       " 'awake',\n",
       " 'awaken',\n",
       " 'award',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'axel',\n",
       " 'ay',\n",
       " 'aye',\n",
       " 'ayo',\n",
       " 'ayy',\n",
       " 'az',\n",
       " 'azealia',\n",
       " 'azul',\n",
       " 'ba',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babymomma',\n",
       " 'bach',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backless',\n",
       " 'backs',\n",
       " 'backseat',\n",
       " 'backslash',\n",
       " 'backstreet',\n",
       " 'backup',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badda',\n",
       " 'badder',\n",
       " 'baddest',\n",
       " 'baddiebey',\n",
       " 'badibadee',\n",
       " 'badonkadonk',\n",
       " 'bae',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'baggy',\n",
       " 'bags',\n",
       " 'bahamas',\n",
       " 'bail',\n",
       " 'baila',\n",
       " 'bailando',\n",
       " 'bailey',\n",
       " 'baileys',\n",
       " 'baiser',\n",
       " 'bajan',\n",
       " 'bajans',\n",
       " 'bajo',\n",
       " 'bajun',\n",
       " 'baking',\n",
       " 'bal',\n",
       " 'balance',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'ballers',\n",
       " 'ballet',\n",
       " 'ballin',\n",
       " 'balloon',\n",
       " 'balls',\n",
       " 'bally',\n",
       " 'bama',\n",
       " 'band',\n",
       " 'bandanas',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'bangin',\n",
       " 'bangkok',\n",
       " 'bank',\n",
       " 'bankhead',\n",
       " 'banks',\n",
       " 'banna',\n",
       " 'banner',\n",
       " 'bans',\n",
       " 'baobab',\n",
       " 'baptize',\n",
       " 'bar',\n",
       " 'barbados',\n",
       " 'barbarian',\n",
       " 'barbeque',\n",
       " 'barber',\n",
       " 'barbie',\n",
       " 'bare',\n",
       " 'barefoot',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'bark',\n",
       " 'barneys',\n",
       " 'barretta',\n",
       " 'barricades',\n",
       " 'bars',\n",
       " 'base',\n",
       " 'basement',\n",
       " 'bask',\n",
       " 'basket',\n",
       " 'basquiat',\n",
       " 'bass',\n",
       " 'bassline',\n",
       " 'basta',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'bathroom',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'bawl',\n",
       " 'bay',\n",
       " 'bayou',\n",
       " 'bb',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beaches',\n",
       " 'beads',\n",
       " 'beams',\n",
       " 'bear',\n",
       " 'beast',\n",
       " 'beasting',\n",
       " 'beat',\n",
       " 'beating',\n",
       " 'beats',\n",
       " 'beaucoup',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'becah',\n",
       " 'becky',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'bedingfield',\n",
       " 'bedpost',\n",
       " 'bedroom',\n",
       " 'bedtime',\n",
       " 'bee',\n",
       " 'beeeeze',\n",
       " 'beef',\n",
       " 'beefs',\n",
       " 'beer',\n",
       " 'bees',\n",
       " 'beeze',\n",
       " 'befall',\n",
       " 'beg',\n",
       " 'begged',\n",
       " 'beggers',\n",
       " 'beggin',\n",
       " 'beggine',\n",
       " 'begin',\n",
       " 'beginnig',\n",
       " 'beginnin',\n",
       " 'behave',\n",
       " 'behind',\n",
       " 'beholder',\n",
       " 'bein',\n",
       " 'beings',\n",
       " 'belief',\n",
       " 'beliefs',\n",
       " 'believe',\n",
       " 'believer',\n",
       " 'believin',\n",
       " 'bell',\n",
       " 'bello',\n",
       " 'bells',\n",
       " 'belong',\n",
       " 'belt',\n",
       " 'belushi',\n",
       " 'bendin',\n",
       " 'beneath',\n",
       " 'benjis',\n",
       " 'benoit',\n",
       " 'bent',\n",
       " 'bentley',\n",
       " 'bentz',\n",
       " 'benz',\n",
       " 'berret',\n",
       " 'berry',\n",
       " 'bes',\n",
       " 'besar',\n",
       " 'beside',\n",
       " 'best',\n",
       " 'besta',\n",
       " 'bestest',\n",
       " 'bestfriends',\n",
       " 'besándote',\n",
       " 'bet',\n",
       " 'betcha',\n",
       " 'bethlehem',\n",
       " 'betray',\n",
       " 'betta',\n",
       " 'bette',\n",
       " 'better',\n",
       " 'bey',\n",
       " 'beyonce',\n",
       " 'beyoncè',\n",
       " 'beyoncé',\n",
       " 'beyond',\n",
       " 'bezel',\n",
       " 'bi',\n",
       " 'bible',\n",
       " 'biddle',\n",
       " 'bien',\n",
       " 'bienvenue',\n",
       " 'bifteck',\n",
       " 'bifton',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'biggie',\n",
       " 'bike',\n",
       " 'bikini',\n",
       " 'bill',\n",
       " 'billi',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'bind',\n",
       " 'binds',\n",
       " 'birds',\n",
       " 'birkin',\n",
       " 'birth',\n",
       " 'birthday',\n",
       " 'birthing',\n",
       " 'birthright',\n",
       " 'bisbal',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitches',\n",
       " 'bite',\n",
       " 'bitter',\n",
       " 'bittersweet',\n",
       " 'bitty',\n",
       " 'black',\n",
       " 'blackbird',\n",
       " 'blackjack',\n",
       " 'blackness',\n",
       " 'blacks',\n",
       " 'bladder',\n",
       " 'blade',\n",
       " 'blague',\n",
       " 'blah',\n",
       " 'blahnik',\n",
       " 'blaine',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blast',\n",
       " 'blaze',\n",
       " 'blazer',\n",
       " 'blazin',\n",
       " 'blazing',\n",
       " 'ble',\n",
       " 'bleach',\n",
       " 'bleaching',\n",
       " 'blee',\n",
       " 'bleed',\n",
       " 'bleek',\n",
       " 'blend',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessin',\n",
       " 'blessing',\n",
       " 'blessings',\n",
       " 'blew',\n",
       " 'blige',\n",
       " 'blind',\n",
       " 'blinded',\n",
       " 'blindfold',\n",
       " 'blindly',\n",
       " 'blindés',\n",
       " 'bliss',\n",
       " 'blisters',\n",
       " 'blizzards',\n",
       " 'block',\n",
       " 'blocks',\n",
       " 'bloggers',\n",
       " 'blogs',\n",
       " 'blonder',\n",
       " 'blood',\n",
       " 'bloodline',\n",
       " 'bloom',\n",
       " 'bloomberg',\n",
       " 'blouse',\n",
       " 'blow',\n",
       " 'blowin',\n",
       " 'blowing',\n",
       " 'blubland',\n",
       " 'blue',\n",
       " 'bluebirds',\n",
       " 'blues',\n",
       " 'bluff',\n",
       " 'blunt',\n",
       " 'blush',\n",
       " 'bmw',\n",
       " 'bo',\n",
       " 'board',\n",
       " 'boards',\n",
       " 'boat',\n",
       " 'bobbly',\n",
       " 'bobby',\n",
       " 'bodied',\n",
       " 'bodies',\n",
       " 'body',\n",
       " 'boi',\n",
       " 'boilin',\n",
       " 'bold',\n",
       " 'boma',\n",
       " 'bomaye',\n",
       " 'bomb',\n",
       " 'bombs',\n",
       " 'bon',\n",
       " 'bonds',\n",
       " 'bone',\n",
       " 'boner',\n",
       " 'bones',\n",
       " 'boney',\n",
       " 'bonheurs',\n",
       " 'bonjour',\n",
       " 'bonnie',\n",
       " 'bonny',\n",
       " 'bono',\n",
       " 'bony',\n",
       " 'boo',\n",
       " 'boogie',\n",
       " 'boojy',\n",
       " 'book',\n",
       " 'books',\n",
       " 'boom',\n",
       " 'boomers',\n",
       " 'boost',\n",
       " 'boots',\n",
       " 'booty',\n",
       " 'bootyb',\n",
       " 'bootylicious',\n",
       " 'boppers',\n",
       " 'border',\n",
       " 'borders',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'bossy',\n",
       " 'bother',\n",
       " 'bottle',\n",
       " 'bottles',\n",
       " 'bottom',\n",
       " 'bottomless',\n",
       " 'boucher',\n",
       " 'boudin',\n",
       " 'boudoir',\n",
       " 'bouffes',\n",
       " 'bouger',\n",
       " 'boulevard',\n",
       " 'bounce',\n",
       " 'bouncin',\n",
       " 'bouncing',\n",
       " 'boundless',\n",
       " 'bount',\n",
       " 'bouquet',\n",
       " 'bout',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyboy',\n",
       " 'boyfirend',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'boyy',\n",
       " 'boyyyy',\n",
       " 'bp',\n",
       " 'bra',\n",
       " 'brace',\n",
       " 'bracing',\n",
       " 'brad',\n",
       " 'braggin',\n",
       " 'braid',\n",
       " 'braids',\n",
       " 'brain',\n",
       " 'brains',\n",
       " 'brand',\n",
       " 'brander',\n",
       " 'brando',\n",
       " 'brap',\n",
       " 'brat',\n",
       " 'brave',\n",
       " 'brawls',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakdown',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakups',\n",
       " 'breastese',\n",
       " 'breate',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breathin',\n",
       " 'breathing',\n",
       " 'breezin',\n",
       " 'brick',\n",
       " 'bricks',\n",
       " 'bridge',\n",
       " 'briefcase',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'brightest',\n",
       " 'brignac',\n",
       " 'brille',\n",
       " 'brilliant',\n",
       " 'brillo',\n",
       " 'brim',\n",
       " 'bring',\n",
       " 'brinks',\n",
       " 'britney',\n",
       " 'broad',\n",
       " 'broads',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bronches',\n",
       " 'bronx',\n",
       " 'brooklyn',\n",
       " 'brotha',\n",
       " 'brothas',\n",
       " 'brother',\n",
       " 'brown',\n",
       " 'browsing',\n",
       " 'bruce',\n",
       " 'bruh',\n",
       " 'bruk',\n",
       " 'bruno',\n",
       " 'brunormoliveira',\n",
       " 'brush',\n",
       " 'brushin',\n",
       " 'brutality',\n",
       " 'bub',\n",
       " 'bubble',\n",
       " 'bubbles',\n",
       " 'buck',\n",
       " 'buckle',\n",
       " 'buddha',\n",
       " 'buddy',\n",
       " 'buds',\n",
       " 'bue',\n",
       " 'bug',\n",
       " 'bugatti',\n",
       " 'bugging',\n",
       " 'build',\n",
       " 'buildings',\n",
       " 'bullet',\n",
       " 'bulletproof',\n",
       " 'bullets',\n",
       " 'bulls',\n",
       " 'bully',\n",
       " 'bum',\n",
       " 'bumm',\n",
       " 'bump',\n",
       " 'bumpin',\n",
       " 'bun',\n",
       " 'bunny',\n",
       " 'burberry',\n",
       " 'burbs',\n",
       " 'burden',\n",
       " 'bureaux',\n",
       " 'burn',\n",
       " 'burnin',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burress',\n",
       " 'burst',\n",
       " 'bury',\n",
       " 'bus',\n",
       " 'buscar',\n",
       " 'buscaré',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'buss',\n",
       " 'bust',\n",
       " 'busta',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'butta',\n",
       " 'butter',\n",
       " 'butterflies',\n",
       " 'button',\n",
       " 'buttons',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bvalgari',\n",
       " 'bwoy',\n",
       " 'bye',\n",
       " 'byeee',\n",
       " 'ca',\n",
       " 'caan',\n",
       " 'cab',\n",
       " 'cabeza',\n",
       " 'cabinets',\n",
       " 'cable',\n",
       " 'cada',\n",
       " 'caddy',\n",
       " 'cadillac',\n",
       " 'caen',\n",
       " 'caer',\n",
       " 'caesar',\n",
       " 'cage',\n",
       " 'cah',\n",
       " 'cain',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callar',\n",
       " 'callin',\n",
       " 'calling',\n",
       " 'callo',\n",
       " 'calls',\n",
       " 'calm',\n",
       " 'calma',\n",
       " 'calmin',\n",
       " 'calvin',\n",
       " 'calypso',\n",
       " 'cameo',\n",
       " 'camera',\n",
       " 'cameras',\n",
       " 'camino',\n",
       " 'camo',\n",
       " 'campbell',\n",
       " 'camry',\n",
       " 'can',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'canción',\n",
       " 'candelabra',\n",
       " 'candidate',\n",
       " 'candle',\n",
       " 'candles',\n",
       " 'candy',\n",
       " 'cane',\n",
       " 'caniveau',\n",
       " 'cannonball',\n",
       " 'canse',\n",
       " 'cap',\n",
       " 'capacity',\n",
       " 'capaz',\n",
       " 'cape',\n",
       " 'capital',\n",
       " 'capricorn',\n",
       " 'capricorns',\n",
       " 'captain',\n",
       " 'captivate',\n",
       " 'capture',\n",
       " 'car',\n",
       " 'cara',\n",
       " 'carbon',\n",
       " 'card',\n",
       " 'cardiac',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'cares',\n",
       " 'carey',\n",
       " 'carline',\n",
       " 'carlo',\n",
       " 'carlos',\n",
       " 'carmen',\n",
       " 'carriage',\n",
       " 'carribean',\n",
       " 'carrie',\n",
       " 'carry',\n",
       " 'cars',\n",
       " 'carter',\n",
       " 'carters',\n",
       " 'cartier',\n",
       " 'carve',\n",
       " 'casanova',\n",
       " 'cascade',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'casino',\n",
       " 'casket',\n",
       " 'caskets',\n",
       " 'cast',\n",
       " 'castle',\n",
       " 'castles',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catchin',\n",
       " 'category',\n",
       " 'catfishes',\n",
       " 'cats',\n",
       " 'catwalk',\n",
       " 'cause',\n",
       " 'cavalier',\n",
       " 'cave',\n",
       " 'cavities',\n",
       " 'cayendo',\n",
       " 'cayó',\n",
       " 'caímos',\n",
       " 'caïds',\n",
       " 'cd',\n",
       " 'ce',\n",
       " 'cee',\n",
       " 'ceed',\n",
       " 'ceiling',\n",
       " 'ceilings',\n",
       " 'celebrate',\n",
       " 'celebration',\n",
       " 'celebrity',\n",
       " 'celie',\n",
       " 'cell',\n",
       " 'cellphone',\n",
       " 'center',\n",
       " 'centigrade',\n",
       " 'central',\n",
       " 'century',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'certify',\n",
       " 'ces',\n",
       " 'ceux',\n",
       " 'cha',\n",
       " 'chain',\n",
       " 'chains',\n",
       " 'chair',\n",
       " 'chaka',\n",
       " 'challenge',\n",
       " 'challenger',\n",
       " 'challengers',\n",
       " 'cham',\n",
       " 'champa',\n",
       " 'champagne',\n",
       " 'champion',\n",
       " 'champions',\n",
       " 'chance',\n",
       " 'chances',\n",
       " 'chandeliers',\n",
       " 'chanel',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changent',\n",
       " 'changes',\n",
       " 'channel',\n",
       " 'chaque',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names() #Todas as palavras do nosso vocabulário "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsoJ7CQkPW7G",
    "outputId": "8fc4c0e3-ee08-4c2c-ed37-c7e7f7d23898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3627"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get('love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTT5x5WLPW7M"
   },
   "source": [
    "Exemplo da nossa matriz termo-documento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpWLbwtIPW7N",
    "outputId": "4c313714-dbac-4f2d-99b5-2893b8ab92ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaaah</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahhhh</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abanenkani</th>\n",
       "      <th>abaziyo</th>\n",
       "      <th>abit</th>\n",
       "      <th>...</th>\n",
       "      <th>égaux</th>\n",
       "      <th>élever</th>\n",
       "      <th>élu</th>\n",
       "      <th>éléverons</th>\n",
       "      <th>état</th>\n",
       "      <th>été</th>\n",
       "      <th>évite</th>\n",
       "      <th>évoque</th>\n",
       "      <th>êt</th>\n",
       "      <th>única</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7063 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaaaaah  aaah  aah  aahhhh  aaron  abandon  abanenkani  abaziyo  abit  \\\n",
       "0   0        0     0    0       0      0        0           0        0     0   \n",
       "1   0        0     0    0       0      0        0           0        0     0   \n",
       "2   0        0     0    0       0      0        0           0        0     0   \n",
       "3   0        0     0    0       0      0        0           0        0     0   \n",
       "4   0        0     0    0       0      0        0           0        0     0   \n",
       "\n",
       "   ...  égaux  élever  élu  éléverons  état  été  évite  évoque  êt  única  \n",
       "0  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "1  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "2  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "3  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "4  ...      0       0    0          0     0    0      0       0   0      0  \n",
       "\n",
       "[5 rows x 7063 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame(X.toarray(), columns = count_vectorizer.get_feature_names())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQasqKkLPW7g"
   },
   "source": [
    "No dataframe acima, cada uma das colunas representa uma das palavras do nosso vocabulário, e cada linha, um dos nossos documentos, ou seja, uma das nossas músicas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGmN5MILPW8A"
   },
   "source": [
    "## Separando em Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wieWN86PW8C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.toarray()\n",
    "y = df['artista']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V03QPr0QPW8H"
   },
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJKMuSYLPW8J"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Criando o Modelo Naive Bayes \n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#.......Treinando o Modelo.......\n",
    "naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "#Fazendo as previsões\n",
    "naive_bayes_pred = naive_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJNecXsIPW8f"
   },
   "source": [
    "## Métricas \n",
    "Após estarmos com nosso modelo de classificação pronto, devemos avaliá-lo e, para isso, utilizamos as métricas de classificação. \n",
    "### Matriz de confusão\n",
    "Primeiro, quando estamos lidando com um modelo cuja target é categórica (como no nosso caso, em que as músicas pertencem ou a Beyoncé ou a Rihanna), podemos utilizar uma matriz de confusão para analisarmos melhor onde o nosso modelo está acertando e onde ele está errando. Ela apresenta o seguinte formato:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Fabio_Araujo_Da_Silva/publication/323369673/figure/fig5/AS:597319787479040@1519423543307/Figura-13-Exemplo-de-uma-matriz-de-confusao.png\" alt=\"Exemplo de uma matriz de confusão\"/></a>\n",
    "\n",
    "Na vertical, estão indicados os valores previstos pelo modelo e, na horizontal, os valores reais. Para cada elemento da matriz, temos dois valores associados: o previsto e o real. Se esse valores coincidirem, tem-se uma previsão correta/verdadeira (por exemplo, verdadeiros positivos e verdadeiros negativos, que estão em verde na imagem). Caso contrário, tem-se um erro cometido pelo modelo (como ocorre nos quadrados vermelhos da imagem acima). \n",
    "\n",
    "### Acurácia\n",
    "A acurácia é, basicamente, uma métrica que indica a relação entre quanto o seu modelo acertou do quanto ele avaliou. Considerando a matriz de confusão mostrada, a acurácia seria igual à soma dos verdadeiros positivos com os verdadeiros negativos dividida pelo total (soma dos verdadeiros e falsos positivos e negativos). A acurácia não é uma boa métrica a ser utilizada quando analisamos dados desbalanceados, porque pode acontecer de o modelo prever muito bem o evento mais usual e ser péssimo prevendo o evento raro. Assim, como trata-se de uma média simples de acertos pelo total, a grande quantidade de acertos na previsão do evento mais usual compensaria a baixa taxa de acerto do evento raro, resultando em uma acurácia alta que não reflete corretamente a qualidade de predição do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi75e7PvPW8g",
    "outputId": "01433652-7bb5-44d7-ecb9-36128ee22e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo 0.7307692307692307\n",
      "\n",
      "Matriz de confusão: \n",
      " [[34  8]\n",
      " [20 42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#Calculando a acurácia\n",
    "acc = accuracy_score(naive_bayes_pred, y_test)\n",
    "\n",
    "#Matriz de confusão \n",
    "cm = confusion_matrix(naive_bayes_pred, y_test)\n",
    "\n",
    "print(\"Acurácia do modelo\", acc)\n",
    "print(\"\\nMatriz de confusão: \\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6mo3KiPPW8w"
   },
   "source": [
    "## Avaliando as músicas\n",
    "Agora, você pode tentar testar o seu modelo com alguma frase e ver a qual cantora ela se assemelha mais: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjPysbIzPW82",
    "outputId": "d02ed419-177e-44b5-b440-fcf16aa86c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beyoncé']\n"
     ]
    }
   ],
   "source": [
    "nova_frase = [\"i am a single lady\"] \n",
    "teste = count_vectorizer.transform(nova_frase)\n",
    "pred = naive_bayes.predict(teste)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "classificador.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
